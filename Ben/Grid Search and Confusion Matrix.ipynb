{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def fit(self, X, y, descent = \"stochastic\", n_iter = 100000, learning_rate = 0.1, lossConvergence = 0.01, batch_size = 32):\n",
    "        X = X.values\n",
    "        intercept_col = np.ones(X_train.shape[0]).reshape(X_train.shape[0],1)\n",
    "        X = np.hstack((intercept_col, X))\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "         \n",
    "        if descent == \"stochastic\":\n",
    "            self._b = self._stochastic_gradient_descent(X,\n",
    "                                                        y,\n",
    "                                                        n_iter = n_iter,\n",
    "                                                        learning_rate = learning_rate,\n",
    "                                                        lossConvergence = lossConvergence,\n",
    "                                                        batch_size = batch_size)\n",
    "        else:\n",
    "             self._b = self._gradient_descent(X,\n",
    "                                             y,\n",
    "                                             n_iter = n_iter,\n",
    "                                             learning_rate = learning_rate)           \n",
    "        \n",
    "    # Stochastic Gradient Descent Algorithm for Logistic Regression \n",
    "    def _stochastic_gradient_descent(self, X, y, n_iter = 10000, learning_rate = 0.01, lossConvergence = .001, batch_size = 128):\n",
    "        y = y.values.reshape(len(y),1)\n",
    "        betas = np.zeros(X.shape[1]).reshape(X.shape[1],1) + 0.1\n",
    "        for i in range(n_iter):\n",
    "            X_index = np.arange(X.shape[0])\n",
    "            np.random.shuffle(X_index)\n",
    "            batch_index = X_index[:batch_size]\n",
    "\n",
    "            X_batch = X[batch_index,:]\n",
    "            y_batch = y[batch_index,:]\n",
    "\n",
    "            yhat = self._sigmoid(np.dot(X_batch, betas))\n",
    "            gradient = np.dot(X_batch.T,(y_batch - yhat))\n",
    "            if self._loss(yhat, y_batch) < lossConvergence:\n",
    "                break\n",
    "            betas +=  learning_rate * (gradient/X.shape[1])\n",
    "        return betas\n",
    "    \n",
    "   # Gradient Descent Algorithm for Logistic Regression \n",
    "    def _gradient_descent(self, X,y, n_iter = 10000, learning_rate = 0.01, lossConvergence = .001):\n",
    "        y = y.values.reshape(len(y),1)\n",
    "        betas = np.zeros(X.shape[1]).reshape(X.shape[1],1) + 0.1\n",
    "        for i in range(n_iter):\n",
    "            yhat = self._sigmoid(np.dot(X, betas))\n",
    "            gradient = np.dot(X.T,(y - yhat))\n",
    "            if self._loss(yhat, y) < lossConvergence:\n",
    "                break\n",
    "            betas +=  learning_rate * (gradient/X.shape[1])\n",
    "        return X, betas\n",
    "    \n",
    "    def _loss(self, yhat, y):\n",
    "        loss_vals = yhat.copy()\n",
    "        loss_vals[y == 1] = -np.log(yhat[y==1])\n",
    "        loss_vals[y == 0] = -np.log(1 - yhat[y==0])\n",
    "        return(loss_vals.mean())\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = X.values\n",
    "        intercept_col = np.ones(X_train.shape[0]).reshape(X_train.shape[0],1)\n",
    "        X = np.hstack((intercept_col, X))\n",
    "        return self._sigmoid(np.dot(X, self._b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    \n",
    "    def __init__(self, yhat, y):\n",
    "        yhat_copy = yhat.copy()\n",
    "        yhat_copy[yhat >= 0.5] = 1\n",
    "        yhat_copy[yhat < 0.5] = 0\n",
    "\n",
    "        true_positive_num = 0\n",
    "        false_positive_num = 0\n",
    "        false_negative_num = 0\n",
    "        true_negative_num = 0\n",
    "\n",
    "        for yhat_i, y_i in zip(yhat_copy, y):\n",
    "            if yhat_i == 1 and y_i == 1:\n",
    "                true_positive_num += 1\n",
    "            elif yhat_i == 1 and y_i == 0:\n",
    "                false_positive_num += 1\n",
    "            elif yhat_i == 0 and y_i == 0:\n",
    "                true_negative_num += 1\n",
    "            elif yhat_i == 0 and y_i == 1:\n",
    "                false_negative_num += 1\n",
    "\n",
    "        t = len(y)\n",
    "        \n",
    "        self.true_positive = np.round(true_positive_num/t,2)\n",
    "        self.false_positive = np.round(false_positive_num/t,2)\n",
    "        self.false_negative = np.round(false_negative_num/t,2)\n",
    "        self.true_negative = np.round(true_negative_num/t,2)\n",
    "\n",
    "        self.df = pd.DataFrame([[self.true_positive,self.false_positive], [self.false_negative, self.true_negative]])\n",
    "        self.df = self.df.rename(columns = {0: \"Positive\", 1: \"Negative\"}, index = {0: \"Positive\", 1: \"Negative\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses stochastic gradient decent at the moment\n",
    "# Assumes the following variables are defined:\n",
    "\n",
    "# X_train_subset_tfidf - tfidf train set\n",
    "# X_test_subset_tfidf - tfidf test set\n",
    "\n",
    "# y - matrix of 12,500 0s and 12,500 1s\n",
    "\n",
    "def logistic_regression_grid_search(n_iter_list, learning_rate_list, lossConvergence_list, batch_size_list):\n",
    "    \n",
    "    unique_regressions = []\n",
    "    \n",
    "    logistic_regression = LogisticRegression()\n",
    "    \n",
    "    combinations = itertools.product(n_iter_list, learning_rate_list, lossConvergence_list, batch_size_list)\n",
    "    \n",
    "    for combination in combinations:\n",
    "        print(\"Running combination:\")\n",
    "        n_iter, learning_rate, lossConvergence, batch_size = combination\n",
    "        \n",
    "        print(\"n_iter: \", n_iter)\n",
    "        print(\"learning rate: \", learning_rate)\n",
    "        print(\"lossConvergence: \", lossConvergence)\n",
    "        print(\"batch size: \", batch_size)\n",
    "        \n",
    "        logistic_regression.fit(X_train_subset_tfidf, y_train[\"Sentiment\"], descent= \"stochastic\", n_iter=n_iter, learning_rate=learning_rate, lossConvergence=lossConvergence, batch_size = batch_size)\n",
    "        \n",
    "        yhat_lr_test = logistic_regression.predict(X_test_subset_tfidf)\n",
    "        \n",
    "        cm = ConfusionMatrix(yhat_lr_test, y)\n",
    "        \n",
    "        print(\"Data Frame:\")\n",
    "        print(cm.df)\n",
    "        \n",
    "        print(\"Accuracy:\", cm.true_positive + cm.true_negative)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        unique_regressions.append((combination, cm)) # Tuple of combination and confusion matrix\n",
    "                        \n",
    "    return unique_regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_list = [50000, 100000, 200000]\n",
    "learning_rate_list = [1]\n",
    "lossConvergence_list = [0.01, 0.05]\n",
    "batch_size_list = [512, 1024, 2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_regressions = logistic_regression_grid_search(n_iter_list, learning_rate_list, lossConvergence_list, batch_size_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
